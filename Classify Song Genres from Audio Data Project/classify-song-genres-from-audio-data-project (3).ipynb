{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom scipy import stats\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Examine CSV Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"rock_hiphop_data = pd.read_csv('../input/rockvshiphop/fma-rock-vs-hiphop.csv', index_col='track_id')\nrock_hiphop_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing CSV Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns with null data\nrock_hiphop_data.columns[rock_hiphop_data.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping 'composer', 'information', 'lyricist', 'publisher' columns (with null values)\n# + dropping 'tags', and 'title'\nrock_hiphop_data.drop(['composer', 'information', 'lyricist', 'publisher', 'tags', 'title'], axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling 'date_recorded' with the same value as the 'date_created'\nrock_hiphop_data['date_recorded'].fillna(rock_hiphop_data.date_created, inplace=True)\nrock_hiphop_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating 'year_created', 'year_recorded', 'recording_period' columns\nrock_hiphop_data['date_created'] = pd.to_datetime(rock_hiphop_data['date_created'], format=\"%Y %m %d %H\")\nrock_hiphop_data['year_created'] = rock_hiphop_data.date_created.dt.year\n\nrock_hiphop_data['date_recorded'] = pd.to_datetime(rock_hiphop_data['date_recorded'], format=\"%Y %m %d %H\")\nrock_hiphop_data['year_recorded'] = rock_hiphop_data.date_recorded.dt.year\n\nrock_hiphop_data['recording_period'] = (rock_hiphop_data.date_created - rock_hiphop_data.date_recorded).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rock_hiphop_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping 'date_created', 'date_recorded'\nrock_hiphop_data.drop(['date_created', 'date_recorded'], axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling 'language_code', and 'license' backward then forward\nrock_hiphop_data['language_code'] = rock_hiphop_data.language_code.bfill().ffill()\nrock_hiphop_data['license'] = rock_hiphop_data.license.bfill().ffill()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting genres_all\nimport re\n# get numbers from unique strings\nlist_genres_all = [re.findall('\\d+', l) for l in rock_hiphop_data.genres_all.unique()]\n# convert them to int, flatten list of lists, set of unique integers\nflatten_genres_all = np.unique([int(item) for sublist in list_genres_all for item in sublist])\nflatten_genres_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating columns for different 36 genres\nfor i in range(len(flatten_genres_all)):\n    rock_hiphop_data[f\"genres_{flatten_genres_all[i]}\"] = rock_hiphop_data.genres_all.map(lambda a: 1 if (str(flatten_genres_all[i]) in re.findall('\\d+', a)) else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rock_hiphop_data.genres_811.tail(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping 'genres', 'genres_all'\nrock_hiphop_data.drop(['genres', 'genres_all'], axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Examine JSON Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f = open('../input/rockvshiphop/echonest-metrics.json',) \n  \n# returns JSON object as  \n# a dictionary \nmetrics_data = json.load(f) \n  \nf.close() \n\nmetrics_df = pd.DataFrame.from_dict(metrics_data).set_index('track_id')\nmetrics_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing JSON Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns with null data\nmetrics_df.columns[metrics_df.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Joining All Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# joining data with 'track_id', rock_hiphop_data on the left, and metrics_df on the right\nall_data = rock_hiphop_data.join(metrics_df)\nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing All Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns with null data\nall_data.columns[all_data.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.bfill(inplace=True)\nall_data.ffill(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.columns[all_data.isna().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After getting **100%** from the **Decision Tree Model** and **99%** from the **Logistic Regression Model** (after scaling and applying PCA), I decided to drop the most correlated columns: 'genres_12', and 'genres_21', to prevent overfitting with new test data!!"},{"metadata":{},"cell_type":"markdown","source":"So, Now I have got \n* **98%** from **Decision Tree Model** on the splitted data (before scaling and applying PCA), \n* **47%** from **Decision Tree Model** on the cross validated data (after scaling),\n* **83%** from **Logistic Regression Model** on the splitted data (before scaling and applying PCA),\n* **94.6%** from **Logistic Regression Model** on the cross validated data (after scaling)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping 'genres_12', 'genres_21'\nall_data.drop(['genres_12', 'genres_21'], axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding the Target Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply label encoder \nlabel_encoder = LabelEncoder()\nall_data.genre_top = label_encoder.fit_transform(all_data.genre_top)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split to All Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove rows with missing target, separate target from predictors\nX = all_data.dropna(axis=0, subset=['genre_top'])\ny = all_data.genre_top\nX.drop(['genre_top'], axis=1, inplace=True)\n# Scaling Features\n#X_scaled = X.loc[:, X.columns]\n#X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Categorical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seeing if we can apply Label Encoding\n# All categorical columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train[col]) == set(X_valid[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many columns are needed to one-hot encode \nnum_cols_language_code = X_train['language_code'].nunique()\nprint(num_cols_language_code)\n\n# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 15]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Use as many lines of code as you need!\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nX_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nX_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trying Decision Tree and Logistic Regression before Scaling/Normalizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DecisionTreeRegressor without Scaling\nfrom sklearn.metrics import mean_absolute_error\n\ndt_model = DecisionTreeRegressor(random_state=1)\ndt_model.fit(X_train, y_train)\n\n# Save the model\npickle.dump(dt_model, open('./decision_tree_model_all.sav', 'wb'))\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = dt_model.predict(X_valid)\n\nval_mae = mean_absolute_error(val_predictions, y_valid)\nprint(\"Validation MAE for Decision Tree: {:,.5f}\".format(val_mae))\n\ncm = confusion_matrix(y_valid, val_predictions)\nprint(cm)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.imshow(cm)\nax.grid(False)\nax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\nax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\nax.set_ylim(1.5, -0.5)\nfor i in range(2):\n    for j in range(2):\n        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\nplt.title(\"Confusion Matrix of Decision Tree Model Validation\")\nplt.show()\n\nprint(classification_report(y_valid, val_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LogisticRegression without Scaling\nlr_model = LogisticRegression(solver='liblinear', random_state=0)\nlr_model.fit(X_train, y_train)\nval_predictions = lr_model.predict(X_valid)\n\nval_mae = mean_absolute_error(val_predictions, y_valid)\nprint(\"Validation MAE for Logistic Regression: {:,.5f}\".format(val_mae))\n\ncm = confusion_matrix(y_valid, val_predictions)\nprint(cm)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.imshow(cm)\nax.grid(False)\nax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\nax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\nax.set_ylim(1.5, -0.5)\nfor i in range(2):\n    for j in range(2):\n        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\nplt.title(\"Confusion Matrix of Logistic Regression Model Validation\")\nplt.show()\n\nprint(classification_report(y_valid, val_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalizing the Feature Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling Features\nX_scaled = X.loc[:, X.columns]\nX_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get Mutual Information"},{"metadata":{},"cell_type":"markdown","source":"### to select features to use in further analyses of PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mi_scores = make_mi_scores(X_scaled, y)\nmi_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.yticks(fontsize=20)\n\nplot_mi_scores(mi_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\n #   \"genres_12\",\n #   \"genres_21\",\n    \"speechiness\",\n    'danceability',\n    'tempo',\n #   'acousticness',\n    'energy',\n    'instrumentalness',\n    'valence'\n]\n\nprint(\"Correlation with genre_top:\\n\")\nprint(X_scaled[features].corrwith(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.copy()\nX = X.loc[:, features]\n\n# `apply_pca`, defined above, reproduces the code from the tutorial\npca, X_pca, loadings = apply_pca(X)\nprint(loadings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying Decision Tree and Logistic Regression on PCA columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_dataset(X, y, model):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()), ('model', model)])\n    \n    scoring = [\"accuracy\",\"roc_auc\",\"neg_log_loss\",\"r2\",\n             \"neg_mean_squared_error\",\"neg_mean_absolute_error\"] \n    \n    score = cross_val_score(\n        my_pipeline, X, y, cv=5, scoring=\"accuracy\",\n    )\n    score = score.mean()#-1 * score.mean()\n    #score = np.sqrt(score)\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_scaled = X_scaled.join(X_pca)\nscore = score_dataset(X_scaled, y, DecisionTreeRegressor(random_state=0))\nprint(f\"Decision Tree score: {score:.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = score_dataset(X_scaled, y, LogisticRegression(solver='liblinear', random_state=0))\nprint(f\"Logistic Regression score: {score:.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}